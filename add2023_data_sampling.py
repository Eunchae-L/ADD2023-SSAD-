# -*- coding: utf-8 -*-
"""ADD2023_Data_Sampling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RBowfkThYtdd5Pd0-Vj-q0-mnZvy341m
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/STUDY/Project/ADD2023

import os
import random
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from typing import List, Tuple

# =========================
# Global Config
# =========================
SR = 16000
CHUNK_SEC = 2.0
CHUNK_SAMPLES = int(SR * CHUNK_SEC)
MIN_GAP_SEC = 1.0

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# stage ratio
STAGE1_RATIO = 0.35
STAGE2_RATIO = 0.65

# Duration bin ratios
DURATION_RATIOS = {
    "2-4": 0.30,
    "4-8": 0.30,
    "8-15": 0.25,
    "15+": 0.15
}

META_CSV = "data/Track1.2/add2023_t1.2_train_metadata.csv"
OUT_ROOT = "data/Track1.2/train/chunked"

# Target counts
TARGET = {
    "stage1": {"fake": 8100, "genuine": 1000},
    "stage2": {"fake": 8450, "genuine": 8450},
}

VALID_BINS = ["2-4s", "4-8s", "8-15s"]

"""Utils"""

#chunk 후보 생성
def generate_chunks(duration):
  max_start = duration - CHUNK_SEC
  if max_start <= 0:
    return []
  candidates = np.arange(0, max_start + 1e-6, MIN_GAP_SEC).tolist()
  return candidates

meta = pd.read_csv(META_CSV)
meta = meta[meta.length_bin.isin(VALID_BINS)]
meta = meta.sample(frac=1, random_state=SEED).reset_index(drop=True)

"""meta.sample(frac=1, random_state=SEED)

===> sample()
DataFrame에서 행을 무작위로 샘플링하는 함수입니다.

===> frac=1
전체 데이터의 100%를 샘플링하겠다는 의미입니다.
즉, 행을 줄이지 않고 순서만 랜덤하게 섞습니다 (shuffle).

.reset_index(drop=True)

reset_index()
셔플 후 기존 인덱스가 뒤섞여 있으므로, 이를 0부터 다시 연속적으로 부여합니다.

drop=True
기존 인덱스를 새로운 컬럼으로 남기지 않고 완전히 제거합니다.
(drop=False면 이전 인덱스가 컬럼으로 추가됨)
"""

meta

"""Stage1 Sampling"""

#Stage1-Utterance selection
stage1_meta = []

for label in ["fake", "genuine"]:
  subset = meta[meta.label == label]
  stage1_meta.append(subset.iloc[:TARGET["stage1"][label]])

stage1_meta = pd.concat(stage1_meta).reset_index(drop=True)

used_utterance_stage1 = set(stage1_meta.path_npy.tolist())
used_chunks_stage1 = set() #(path, start_sec)

#Stage1-chunk extraction(1 per utterance)
stage1_chunks = []

for _, row in tqdm(stage1_meta.iterrows(), total=len(stage1_meta), desc="Stage1"):
  starts = generate_chunks(row.duration_sec)
  if not starts:
    continue
  s = random.choice(starts)
  used_chunks_stage1.add((row.path_npy, s))
  stage1_chunks.append({
      "path": row.path_npy,
      "file_id": row.file_id,
      "label": row.label,
      "length_bin": row.length_bin,
      "start_sec": s
  })

"""Stage2"""

#Stage2 Fake - Utterance disjoint
stage2_chunks = []

fake_candidates = meta[
    (meta.label == "fake") &
    (~meta.path_npy.isin(used_utterance_stage1))
]

for _, row in tqdm(fake_candidates.iterrows(), total=len(fake_candidates), desc="Stage2 Fake"):
  if len(stage2_chunks) >= TARGET["stage2"]["fake"]:
    break
  starts = generate_chunks(row.duration_sec)
  if not starts:
    continue
  s = random.choice(starts)
  stage2_chunks.append({
      "path": row.path_npy,
      "file_id": row.file_id,
      "label": row.label,
      "length_bin": row.length_bin,
      "start_sec": s
  })

#Stage2 Genuine
gen_candidates = meta[meta.label == "genuine"]

gen_chunks = []
used_stage2_gen = set()

for _, row in tqdm(gen_candidates.iterrows(), desc="Stage2 Genuine"):
  if len(gen_chunks) >= TARGET["stage2"]["genuine"]:
    break
  starts = generate_chunks(row.duration_sec)
  for s in starts:
    key = (row.path_npy, s)
    if key in used_chunks_stage1 or key in used_stage2_gen:
      continue

    gen_chunks.append({
        "path": row.path_npy,
        "file_id": row.file_id,
        "label": row.label,
        "length_bin": row.length_bin,
        "start_sec": s
    })
    used_stage2_gen.add(key)

    if len(gen_chunks) >= TARGET["stage2"]["genuine"]:
      break

stage2_chunks.extend(gen_chunks)

"""Save"""

def save(chunks, stage):
  out = Path(OUT_ROOT) / stage
  out.mkdir(parents=True, exist_ok=True)

  for c in tqdm(chunks, desc=f"Saving {stage}"):
    x = np.load(c["path"])
    s = int(c["start_sec"] * SR)
    e = s + CHUNK_SAMPLES
    file_id = c["file_id"]
    label = c["label"]
    length_bin = c["length_bin"]
    fname = f"{file_id}_{label}_{length_bin}_{s}.npy"
    np.save(out / fname, x[s:e])

save(stage1_chunks, "stage1")
save(stage2_chunks, "stage2")

#모든 utterance에서 chunk pool 생성
chunk_pool = []

for _, row in tqdm(train_meta.iterrows(), total = len(train_meta), desc="Chunk generation"):
  starts = generate_valid_chunks(row.duration_sec)

  for s in starts:
    chunk_pool.append({
        "path": row.path_npy,
        "file_id": row.file_id,
        "label": row.label,
        "length_bin": row.length_bin,
        "start_sec": s
    })
  chunk_df = pd.DataFrame(chunk_pool).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)

#duration 비율별 sampling
def sample_by_duration(df):
  sampled = []
  total_n = len(df)

  for bin_name, ratio in DURATION_RATIOS.items():
    bin_df = df[df.length_bin == bin_name]
    n = int(total_n * ratio)
    if len(bin_df) == 0:
      continue
    sampled.append(
        bin_df.sample(min(n, len(bin_df)), random_state=RANDOM_SEED)
    )
  return pd.concat(sampled).sample(frac=1, random_state=RANDOM_SEED)

# ======================================================
# 4. Stage split (chunk-level)
# ======================================================
stage1_n = int(len(chunk_df) * STAGE1_RATIO)

stage1_chunks = chunk_df.iloc[:stage1_n].reset_index(drop=True)
stage2_chunks = chunk_df.iloc[stage1_n:].reset_index(drop=True)