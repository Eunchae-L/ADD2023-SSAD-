# -*- coding: utf-8 -*-
"""ADD2023_DataAnalysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p2iGcLx64O8MyeSWyPSB13EAHrZFKGyr
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/STUDY/Project/ADD2023

import os
import pandas as pd
import numpy as np
import librosa
from tqdm import tqdm
import statistics as p

"""# TrainSet"""

target_dir = "data/Track1.2/train/wav"

# 파일 개수 세기
file_count = len([
    f for f in os.listdir(target_dir)
    if os.path.isfile(os.path.join(target_dir, f))
])

print(f"파일 개수: {file_count}개")

label_path = "data/Track1.2/train/label.txt"

labels = {}
with open(label_path, "r") as f:
    for line in f:
        filename, label = line.strip().split()
        labels[filename] = label
print(f"총 {len(labels)}개의 라벨 로드 완료")
print(list(labels.items())[:5])  # 일부 확인

df_labels = pd.read_csv(label_path, sep=" ", header=None, names=["filename", "label"])
print(df_labels.head())
print(df_labels['label'].value_counts())  # fake / genuine 개수 확인

"""## Meta Data Extraction"""

import os
import csv
import numpy as np
from tqdm import tqdm

# =========================
# 설정
# =========================
DATA_ROOT = "data/Track1.2/train"
LABEL_FILE = os.path.join(DATA_ROOT, "label.txt")
NPY_DIR = os.path.join(DATA_ROOT, "librosa")
OUTPUT_CSV = "add2023_t1.2_train_metadata.csv"

DEFAULT_SR = 16000  # librosa load 시 사용한 sr (다르면 수정)

# =========================
# 유틸 함수
# =========================
def get_length_bin(duration):
    if duration < 1.0:
        return "<1s"
    elif duration < 2.0:
        return "1-2s"
    elif duration < 4.0:
        return "2-4s"
    elif duration < 8.0:
        return "4-8s"
    elif duration < 15.0:
        return "8-15s"
    else:
        return "15s+"

# =========================
# label.txt 로드
# =========================
label_map = {}

with open(LABEL_FILE, "r") as f:
    for line in f:
        fname, label = line.strip().split()
        label_map[fname] = label

# =========================
# 메타데이터 생성
# =========================
rows = []

for fname, label in tqdm(label_map.items()):
    npy_path = os.path.join(NPY_DIR, fname.replace(".wav", ".npy"))

    if not os.path.exists(npy_path):
        print(f"[WARN] Missing npy: {npy_path}")
        continue

    try:
        y = np.load(npy_path)
    except Exception as e:
        print(f"[ERROR] Failed to load {npy_path}: {e}")
        continue

    num_samples = len(y)
    duration_sec = num_samples / DEFAULT_SR
    length_bin = get_length_bin(duration_sec)
    num_chunks_2s = int(duration_sec // 2.0)

    rows.append({
        "file_id": fname,
        "path_npy": npy_path,
        "label": label,
        "num_samples": num_samples,
        "sample_rate": DEFAULT_SR,
        "duration_sec": round(duration_sec, 4),
        "length_bin": length_bin,
        "num_chunks_2s": num_chunks_2s,
    })

# =========================
# CSV 저장
# =========================
with open(OUTPUT_CSV, "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=rows[0].keys())
    writer.writeheader()
    writer.writerows(rows)

print(f"Saved metadata CSV: {OUTPUT_CSV}")
print(f"Total files: {len(rows)}")

"""## Label, Silence

### Label 비율 계산
"""

metadata_path = "data/Track1.2/add2023_t1.2_train_metadata.csv"

df = pd.read_csv(metadata_path)

# =========================
# 1) 전체 fake / genuine 비율
# =========================
label_count = df['label'].value_counts()
label_ratio = df['label'].value_counts(normalize=True)

print("=== Label Count ===")
print(label_count)
print()
print("=== Fake / Genuine Ratio ===")
print(label_ratio)
print()

# =========================
# 2) fake 안에서 duration bin 비율
# =========================
fake_df = df[df['label'] == 'fake']
fake_duration_count = fake_df['length_bin'].value_counts()
fake_duration_ratio = (
    fake_df['length_bin']
    .value_counts(normalize=True)
)

print("=== Fake Duration Count ===")
print(fake_duration_count)
print()
print("=== Fake Duration Ratio ===")
print(fake_duration_ratio)
print()

# =========================
# 3) genuine 안에서 duration bin 비율
# =========================
genuine_df = df[df['label'] == 'genuine']
genuine_duration_count = genuine_df['length_bin'].value_counts()
genuine_duration_ratio = (
    genuine_df['length_bin']
    .value_counts(normalize=True)
)

print("=== Genuine Duration Count ===")
print(genuine_duration_count)
print()
print("=== Genuine Duration Ratio ===")
print(genuine_duration_ratio)
print()

"""### Silence 비율 계산

SILENCE_FACTOR=0.1
"""

# CSV_PATH = "data/Track1.2/add2023_t1.2_train_metadata.csv"
# UPDATE_CSV_PATH = "data/Track1.2/Train_Metadata_factor0.1.csv"

# SR = 16000
# FRAME_LENGTH = int(0.025 * SR) # 25 ms
# HOP_LENGTH = int(0.01 * SR) # 10 ms
# SILENCE_FACTOR = 0.1  # relative threshold

# df = pd.read_csv(CSV_PATH)

# silence_durations = []
# silence_ratios = []

# for _, row in tqdm(df.iterrows(), total=len(df)):
#   npy_path = row['path_npy']
#   duration_sec = row["duration_sec"]

#   y = np.load(npy_path)

#   #RMS energy(frame-wise)
#   #rms: shape (num_frames, ) 각 값은 해당 시간 구간의 에너지
#   rms = librosa.feature.rms(
#       y=y,
#       frame_length=FRAME_LENGTH, #한 프레임의 샘플수(25ms)
#       hop_length=HOP_LENGTH, #프레임 간 이동량(10ms)
#       center=True #프레임 중앙 정렬
#   )[0]

#   #relative silence threshold
#   #SILENCE_FACTOR=0.1; 전체 에너지의 중간값보다 10% 이하면 silence로 본다.
#   threshold = p.median(rms) * SILENCE_FACTOR

#   #silence frame 판별
#   silence_frames = rms < threshold
#   silence_frame_count = silence_frames.sum()

#   #시간 변환
#   silence_duration = silence_frame_count * HOP_LENGTH / SR
#   silence_ratio = silence_duration / duration_sec if duration_sec > 0 else 0.0

#   silence_durations.append(round(silence_duration, 4))
#   silence_ratios.append(round(silence_ratio, 4))

# df['silence_duration'] = silence_durations
# df['silence_ratio'] = silence_ratios

# df.to_csv(UPDATE_CSV_PATH, index=False)

# print(f"Saved metadata CSV: {UPDATE_CSV_PATH}")

"""*df.iterrows(): pandas Dataframe을 한 행씩 순회\
*_: row의 index 값
row: 현재 행을 Series 형태로 반환\
*threshold를 계산할 때 mean이 아닌 median을 사용하는 이유: 평균은 큰 발화 에너지에 끌려 올라감. 따라서 silence+speech 가 섞인 분포에서 안정적인 미디언을 사용.\
*  silence_duration = silence_frame_count * HOP_LENGTH / SR
: 각 frame은 hop_length 만큼 시간 간격을 대표함.

Percentile threshold
"""

# import librosa

# CSV_PATH = "data/Track1.2/add2023_t1.2_train_metadata.csv"
# UPDATE_CSV_PATH = "data/Track1.2/Train_Metadata_percentile.csv"

# SR = 16000
# FRAME_LENGTH = int(0.025 * SR) # 25 ms
# HOP_LENGTH = int(0.01 * SR) # 10 ms

# df = pd.read_csv(CSV_PATH)

# silence_durations = []
# silence_ratios = []

# for _, row in tqdm(df.iterrows(), total=len(df)):
#   npy_path = row['path_npy']
#   duration_sec = row["duration_sec"]

#   y = np.load(npy_path)

#   #RMS energy(frame-wise)
#   #rms: shape (num_frames, ) 각 값은 해당 시간 구간의 에너지
#   rms = librosa.feature.rms(
#       y=y,
#       frame_length=FRAME_LENGTH, #한 프레임의 샘플수(25ms)
#       hop_length=HOP_LENGTH, #프레임 간 이동량(10ms)
#       center=True #프레임 중앙 정렬
#   )[0]

#   #relative silence threshold
#   #“에너지 하위 20%는 silence로 보자”
#   threshold = np.percentile(rms, 20)

#   #silence frame 판별
#   silence_frames = rms < threshold
#   silence_frame_count = silence_frames.sum()

#   #시간 변환
#   silence_duration = silence_frame_count * HOP_LENGTH / SR
#   silence_ratio = silence_duration / duration_sec if duration_sec > 0 else 0.0

#   silence_durations.append(round(silence_duration, 4))
#   silence_ratios.append(round(silence_ratio, 4))

# df['silence_duration'] = silence_durations
# df['silence_ratio'] = silence_ratios

# df.to_csv(UPDATE_CSV_PATH, index=False)

# print(f"Saved metadata CSV: {UPDATE_CSV_PATH}")

"""### Silence 분석

히스토그램
"""

csv_path = "data/Track1.2/Train_Metadata_factor0.1.csv"
df = pd.read_csv(csv_path)

df["silence_ratio"].describe()

df["silence_duration"].describe()

import matplotlib.pyplot as plt

csv_path = "data/Track1.2/Train_Metadata_factor0.1.csv"
df = pd.read_csv(csv_path)

# sanity check
assert "silence_ratio" in df.columns
assert "label" in df.columns

# split
df_fake = df[df["label"] == "fake"]
df_genuine = df[df["label"] == "genuine"]

bins = np.linspace(0, 0.5, 50)

plt.figure(figsize=(8, 5))
plt.hist(df["silence_ratio"], bins=bins, alpha=0.5, density=True, label="All")
plt.xlabel("Silence Ratio")
plt.ylabel("Density")
plt.title("All;Silence Ratio Distribution")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.hist(df_fake["silence_ratio"], bins=bins, alpha=0.5, density=True, label="Fake")
plt.xlabel("Silence Ratio")
plt.ylabel("Density")
plt.title("Fake;Silence Ratio Distribution")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.hist(df_genuine["silence_ratio"], bins=bins, alpha=0.5, density=True, label="Genuine")
plt.xlabel("Silence Ratio")
plt.ylabel("Density")
plt.title("Genuine;Silence Ratio Distribution")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

df_fake["silence_ratio"].describe()

df_genuine["silence_ratio"].describe()

# silence_ratio grid
x_grid = np.linspace(0, 0.5, 500)

def empirical_cdf(values, x):
    return np.mean(values <= x)

cdf_fake = np.array([empirical_cdf(df_fake["silence_ratio"].values, x) for x in x_grid])
cdf_genuine = np.array([empirical_cdf(df_genuine["silence_ratio"].values, x) for x in x_grid])

cdf_diff = np.abs(cdf_fake - cdf_genuine)

plt.figure(figsize=(8, 5))

plt.plot(x_grid, cdf_fake, label="Fake CDF")
plt.plot(x_grid, cdf_genuine, label="Genuine CDF")
plt.plot(x_grid, cdf_diff, label="|CDF diff|", linestyle="--", color="black")

plt.xlabel("Silence Ratio Threshold")
plt.ylabel("CDF / Difference")
plt.title("CDF Comparison of Silence Ratio")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 차이 최대 지점
max_diff_idx = np.argmax(cdf_diff)
critical_threshold = x_grid[max_diff_idx]

print(f"Max CDF difference at silence_ratio ≈ {critical_threshold:.3f}")
print(f"Max difference value: {cdf_diff[max_diff_idx]:.3f}")

# diff가 일정 값 이상이 되는 최초 지점
for th in [0.05, 0.10, 0.15, 0.20]:
    idx = np.where(cdf_diff > th)[0]
    if len(idx) > 0:
        print(f"CDF diff > {th:.2f} starts at silence_ratio ≈ {x_grid[idx[0]]:.3f}")

"""# DEV SET"""

target_dir = "data/Track1.2/dev/wav"

# 파일 개수 세기
file_count = len([
    f for f in os.listdir(target_dir)
    if os.path.isfile(os.path.join(target_dir, f))
])

print(f"파일 개수: {file_count}개")

label_path = "data/Track1.2/dev/label.txt"

labels = {}
with open(label_path, "r") as f:
    for line in f:
        filename, label = line.strip().split()
        labels[filename] = label
print(f"총 {len(labels)}개의 라벨 로드 완료")
print(list(labels.items())[:5])  # 일부 확인

df_labels = pd.read_csv(label_path, sep=" ", header=None, names=["filename", "label"])
print(df_labels.head())
print(df_labels['label'].value_counts())  # fake / genuine 개수 확인