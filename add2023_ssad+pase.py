# -*- coding: utf-8 -*-
"""ADD2023_SSAD+PASE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IasLqztH8NyLJ1FlMS6pkdg4pvp7NCxL
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/STUDY/Project/ADD2023

pip install torchcodec --index-url https://download.pytorch.org/whl/cu126

import os
import numpy as np
import librosa
import torch
import torch.nn as nn
import torch.nn.functional as F

import torchaudio
import random

from tqdm import tqdm

class PretrainWaveDataset(torch.utils.data.Dataset):
    def __init__(
        self,
        save_dir,
        chunk_size=32000,
        sr=16000,
        distortion=None,
        apply_distortion=True
      ):
      self.sr = sr
      self.chunk_size = chunk_size
      self.distortion = distortion
      self.apply_distortion = apply_distortion

      self.files = sorted([
          os.path.join(save_dir, f)
          for f in os.listdir(save_dir)
          if f.endswith(".npy")
      ])

    def __len__(self):
      return len(self.files)

    # def _random_crop_or_pad(self, audio):
    #   L = len(audio)

    #   if L >= self.chunk_size:
    #     start = np.random.randint(0, L - self.chunk_size + 1)
    #     return audio[start:start + self.chunk_size]
    #   else:
    #     return np.pad(audio, (0, self.chunk_size - L))

    def _normalize(self, chunk):
      return (chunk - chunk.mean()) / (chunk.std() + 1e-6)

    def __getitem__(self, idx):
      #load waveform
      audio = np.load(self.files[idx])

      # #chunking
      # clean = self._random_crop_or_pad(audio)
      # 안전성 체크 (아주 중요)
      if len(audio) < self.chunk_size:
          audio = np.pad(audio, (0, self.chunk_size - len(audio)))
      else:
          audio = audio[:self.chunk_size]
      #convert to tensor
      clean = torch.tensor(clean).float()

      #distortion(pretraining only)
      if self.apply_distortion and self.distortion is not None:
        distorted = self.distortion(clean)
        distorted = self._normalize(distorted)
      else:
        distorted = clean.clone()

      clean = self._normalize(clean)

      return {
          "clean": clean,
          "distorted": distorted,
          "file_id": os.path.splitext(os.path.basename(self.files[idx]))[0]
      }

class WorkerTargetGenerator:
  def __init__(self,
               sr=16000,
               n_mfcc=40,
               n_fft = 400,
               hop_length = 160,
               wav2vec2_bundle=None,
               device="cpu"):
    self.sr = sr
    self.n_mfcc = n_mfcc
    self.n_fft = n_fft
    self.hop_length = hop_length
    self.device = device
    self.wav2vec2_model = torchaudio.models.Wav2Vec2Model

    # wav2vec2
    if wav2vec2_bundle is not None:
        self.wav2vec2_model = wav2vec2_bundle.get_model().to(device)
        self.wav2vec2_model.eval()
    else:
        self.wav2vec2_model = None

  def compute_mfcc(self, chunk):
    """
        입력 chunk: 1D numpy array of length chunk_size
        반환: torch.Tensor shape=(n_mfcc, T_mfcc)
        """
    mfcc = librosa.feature.mfcc(y=chunk,
                                sr=self.sr,
                                n_mfcc=self.n_mfcc,
                                n_fft=self.n_fft,
                                hop_length=self.hop_length)
    return torch.tensor(mfcc).float()

  def compute_cqt(self, chunk):
    """
    입력 chunk: 1D numpy array of length chunk_size
    반환: torch.Tensor shape=(freq_bins, T_cqt)
    """
    cqt = librosa.cqt(y=chunk, sr=self.sr)
    cqt = np.abs(cqt)
    return torch.tensor(cqt).float()

  def compute_wav2vec2(self, chunk):
    """
    입력 chunk: 1D numpy array
    반환: wav2vec2 feature tensor shape=(T_w2v, feat_dim)
    (frame-level sequence)
    """
    if self.wav2vec2_model is None:
      raise ValueError("wav2vec model is not provided")

    #waveform -> tensor shape (1, chunk_len)
    wav_tensor = torch.tensor(chunk).float().unsqueeze(0).to(self.device)
    proj = torch.nn.Linear(768, 256, bias=False)
    with torch.no_grad():
      features, _ = self.wav2vec2_model.extract_features(wav_tensor)
      # -> features: List[Tensor]
      #Each Tensor is of shape: (batch, time frame, feature dimension)

    #features shape = (1, T_seq, feat_dim)
    mid_layer = features[7].squeeze(0)
    mid_layer = proj(mid_layer)

    return mid_layer

  def get_targets(self, clean_chunk):
    """
    clean_chunk: 1D numpy array or tensor
    반환 딕셔너리: {"mfcc", "cqt", "wav2vec2"}
    """
    #if input is tensor -> numpy array
    if isinstance(clean_chunk, torch.Tensor):
      clean_chunk = clean_chunk.cpu().numpy()

    targets = {
        "mfcc": self.compute_mfcc(clean_chunk),
        "cqt": self.compute_cqt(clean_chunk),
        "wav2vec2": None
    }

    if self.wav2vec2_model is not None:
      targets["wav2vec2"] = self.compute_wav2vec2(clean_chunk)

    return targets

from torch.utils.data import DataLoader

save_root = "pretrain_targets_256dim"
os.makedirs(save_root, exist_ok=True)

# 1) Dataset
dataset = PretrainWaveDataset(
    save_dir="data/Track1.2/train/chunked/stage1",
    apply_distortion=False
)
loader = DataLoader(dataset, batch_size=16, shuffle=True)

# 2) Wav2Vec2 model bundle
bundle = torchaudio.pipelines.WAV2VEC2_BASE

# 3) Target generator
target_gen = WorkerTargetGenerator(
    sr=16000,
    n_mfcc=40,
    wav2vec2_bundle=bundle,
    device="cpu"
)

for batch in tqdm(loader, desc="Generating targets", total=len(loader)):
    clean_batch = batch["clean"]   # tensor (B, 32000)
    file_ids = batch["file_id"]

    for i in range(clean_batch.size(0)):
      id = file_ids[i]
      save_path = os.path.join(save_root, f"{id}.pt")

      if os.path.exists(save_path):
        continue

      # compute targets for first item
      targets = target_gen.get_targets(clean_batch[i])

      torch.save(targets, save_path)

"""저장파일 하나당


{
  "mfcc": Tensor,
  "cqt": Tensor,
  "wav2vec2": Tensor
}

## Speech Distortion

### Reverberation
"""

# %cd /content/drive/MyDrive/STUDY/Project/ADD2023/data/reverb
# !unzip rirs_noises.zip

"""IR 품질 필터링 전처리"""

from scipy.signal import find_peaks

def compute_edc(ir):
  """Schroeder integration (Energy Decay Curve)"""
  energy = ir ** 2
  edc = np.cumsum(energy[::-1])[::-1]
  edc = edc / np.max(edc)
  return 10 * np.log10(edc + 1e-10)

def estimate_t60(edc, sr):
  """Estimate T60 from EDC slope (-5 dB ~ -35 dB)"""
  try:
    t = np.arange(len(edc)) / sr
    idx1 = np.where(edc < -5)[0][0]
    idx2 = np.where(edc < -35)[0][0]
    slope = (edc[idx2] - edc[idx1]) / (t[idx2] - t[idx1])
    t60 = -60 / slope
    return t60
  except:
    return None

def is_monotonic_decreasing(edc, tol=1.0):
  """EDC should not increase significantly"""
  return np.all(np.diff(edc) <= tol)

def validate_ir(ir, sr):
    # normalize
    ir = ir / (np.max(np.abs(ir)) + 1e-9)

    # 1) peak 위치
    peak_idx = np.argmax(np.abs(ir))
    if peak_idx > 0.2 * len(ir):
        return False, None

    # 2) EDC
    edc = compute_edc(ir)
    if not is_monotonic_decreasing(edc):
        return False, None

    # 3) T60
    t60 = estimate_t60(edc, sr)
    if t60 is None or not (0.3 <= t60 <= 0.9):
        return False, None

    return True, t60

#RIRS_NOISES 전체 스캔&필터링
def build_filtered_ir_list(root_dir, target_sr=16000):
    valid_irs = []

    rir_root = os.path.join(root_dir, "simulated_rirs")
    EXCLUDE_NAMES = {"room_info", "rir_list"}

    for room_type in ["smallroom", "mediumroom", "largeroom"]:
        base = os.path.join(rir_root, room_type)

        # room 디렉터리만 선택 + 제외 대상 필터링
        rooms = [
            d for d in os.listdir(base)
            if d not in EXCLUDE_NAMES
            and os.path.isdir(os.path.join(base, d))
        ]

        for room in rooms:
            room_dir = os.path.join(base, room)

            for fn in os.listdir(room_dir):
                if not fn.lower().endswith(".wav"):
                    continue

                path = os.path.join(room_dir, fn)

                ir, sr = torchaudio.load(path)
                ir = ir.mean(dim=0).numpy()  # mono

                if sr != target_sr:
                    ir = F.resample(
                        torch.tensor(ir).unsqueeze(0),
                        sr,
                        target_sr
                    )[0].numpy()

                ok, t60 = validate_ir(ir, target_sr)
                if ok:
                    valid_irs.append({
                        "path": path,
                        "t60": t60,
                        "room_type": room_type,
                        "room": room
                    })

    print(f"Found {len(valid_irs)} valid IRs")
    return valid_irs

import scipy.signal

def apply_reverb(clean, ir_list, sr):
    ir_info = random.choice(ir_list)
    ir, _ = torchaudio.load(ir_info["path"])
    ir = ir.mean(dim=0).numpy()

    out = scipy.signal.fftconvolve(clean, ir, mode="full")[:len(clean)]
    return out / (np.max(np.abs(out)) + 1e-9)

"""### Additive Noise

musan/ \
 ├── noise/ \
 │    ├── sound-bible/ \
 │    └── free-sound/ \
 ├── speech/ \
 └── music/
"""

# %cd /content/drive/MyDrive/STUDY/Project/ADD2023/data/noise
# !tar -xzf musan.tar.gz

"""MUSAN Noise Dataset Loader"""

class MusanNoiseDataset:
  def __init__(self, root_dir, sample_rate=16000):
    self.sample_rate = sample_rate
    self.noise_files = []

    for subdir in ["sound-bible", "free-sound"]:
      subpath = os.path.join(root_dir, subdir)
      for root, _, files in os.walk(subpath):
        for f in files:
          if f.endswith(".wav"):
            self.noise_files.append(os.path.join(root, f))

    assert len(self.noise_files) > 0, "No MUSAN noise wav files found"

  def sample_noise(self, target_len):
    path = random.choice(self.noise_files)
    wav, sr = torchaudio.load(path)

    wav = wav.mean(dim=0) #-> mono

    if sr != self.sample_rate:
      wav = torchaudio.functional.resample(wav, sr, self.sample_rate)

    if wav.numel() < target_len:
      repeat = target_len // wav.numel() + 1
      wav = wav.repeat(repeat)

    #노이즈 전체 중에서 랜덤 시작 지점을 뽑아서 target_len만큼 잘라서 반환
    start = random.randint(0, wav.numel() - target_len)
    noise = wav[start:start + target_len]
    noise = noise / (noise.pow(2).mean().sqrt() + 1e-8)
    return noise

"""SNR기반 Noise Scaling"""

def add_noise_snr(clean, noise, snr_db):
    """
    clean, noise: (T,)
    snr_db: scalar
    """
    clean_energy = clean.pow(2).mean()
    noise_energy = noise.pow(2).mean() + 1e-8

    snr_linear = 10 ** (snr_db / 10)
    scale = torch.sqrt(clean_energy / (snr_linear * noise_energy))

    return clean + scale * noise

"""### Speech Distortion Module"""

from scipy.signal import fftconvolve

class SpeechDistortionModule:
  def __init__(self,
               ir_list,
               noise_dataset,
               sample_rate = 16000,
               p_reverb=0.5,
               p_noise=0.4,
               snr_range=(0,10)):
    self.ir_list = ir_list
    self.noise_dataset = noise_dataset
    self.p_reverb = p_reverb
    self.p_noise = p_noise
    self.snr_range = snr_range
    self.sr = sample_rate

  def apply_reverb(self, clean):
    """
    x: (T,) (시간 축만 있는 mono 음성)
    """
    ir_info = random.choice(self.ir_list)
    ir, _ = torchaudio.load(ir_info["path"])
    ir = ir.mean(dim=0).numpy()

    out = scipy.signal.fftconvolve(
        clean.numpy(), ir, mode="full"
        )[: clean.numel()]
    out = out / (np.max(np.abs(out)) + 1e-9)
    return torch.tensor(out).float()

  def apply_noise(self, clean):
    noise = self.noise_dataset.sample_noise(clean.numel())
    snr = random.uniform(*self.snr_range)
    return add_noise_snr(clean, noise, snr)

  def __call__(self, clean):
    """
    clean: Tensor(T,)
    """
    x = clean.clone()

    if random.random() < self.p_reverb:
      x = self.apply_reverb(x)

    if random.random() < self.p_noise:
      x = self.apply_noise(x)

    #peak normalization
    x = x / (x.abs().max() + 1e-9)
    return x

"""# ENCODER"""

class ConvBlock(nn.Module):
  def __init__(
      self,
      in_ch,
      out_ch,
      kernel_size,
      stride,
      dropout=0.0
  ):
      super().__init__()
      padding = kernel_size // 2

      self.conv = nn.Conv1d(
          in_ch,
          out_ch,
          kernel_size=kernel_size,
          stride=stride,
          padding=padding
      )

      self.bn = nn.BatchNorm1d(out_ch)
      self.act = nn.ReLU()
      #dropout 값이 0보다 크면 Dropout 레이어를 쓰고,0이면 아무 일도 하지 않는 Identity 레이어를 쓴다.
      self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

  def forward(self, x):
      x = self.conv(x)
      x = self.bn(x)
      x = self.act(x)
      x = self.dropout(x)
      return x

class TCNBlock(nn.Module):
  def __init__(
      self,
      channels,
      dilation,
      kernel_size=3): #임의 설정
      super().__init__()
      padding = (kernel_size - 1) * dilation

      self.conv = nn.Conv1d(
          channels,
          channels,
          kernel_size=kernel_size,
          dilation=dilation,
          padding=padding
      )
      self.bn = nn.BatchNorm1d(channels)
      self.act = nn.ReLU()

  def forward(self, x):
      out = self.conv(x)
      out = out[..., : x.size(-1)] #causal crop
      out = self.bn(out)
      out = self.act(out)
      return x + out

class Encoder(nn.Module):
  def __init__(
      self,
      channel_list=(16,32,64,128,128,256,256,512),
      kernel_list=(10,8,8,4,4,4,4,4),
      stride_list=(5,4,2,2,2,2,2,2),
      embedding_dim=256,
      tcn_layers=6,
      proj_hidden=512,
      worker_input_dim=256,
      dropout=0.0
  ):
      super().__init__()
      self.embedding_dim = embedding_dim

      assert len(channel_list) == 8
      assert len(kernel_list) == 8
      assert len(stride_list) == 8

      # 8 Conv1D Block
      convs = [ ]
      in_ch = 1
      for out_ch, kernel_size, stride in zip(channel_list, kernel_list, stride_list):
          convs.append(
              ConvBlock(
                  in_ch,
                  out_ch,
                  kernel_size=kernel_size,
                  stride=stride,
                  dropout=dropout
              )
          )
          in_ch = out_ch
      self.conv_blocks = nn.ModuleList(convs) #nn.ModuleList: 모듈들을 리스트 형태로 관리, 동적으로 모듈 추가/삭제 가능

      # skip projection
      self.skip_proj = nn.ModuleList([
          nn.Conv1d(ch, embedding_dim, kernel_size=1)
          for ch in channel_list
      ])

      # TCN
      tcn = []
      for i in range(tcn_layers):
        tcn.append(
            TCNBlock(
                embedding_dim,
                dilation=2 ** i
            )
        )
      self.tcn = nn.Sequential(*tcn)

      # Nonlinear projection head
      self.proj_head = nn.Sequential(
          nn.Conv1d(embedding_dim, proj_hidden, kernel_size=1),
          nn.ReLU(),
          nn.Conv1d(proj_hidden, worker_input_dim, kernel_size=1)
      )

      self.bn = nn.BatchNorm1d(worker_input_dim)

  def forward(self, x):
      """
      x: (B, T) or (B, 1, T)
      return: (B, worker_input_dim, T)
      """
      if x.dim() == 2:
        x = x.unsqueeze(1)

      skip_feats = []
      out = x

      for block, proj in zip(self.conv_blocks, self.skip_proj):
        out = block(out)
        skip_feats.append(proj(out))

      #temporal alignment (crop to shortest)
      min_t = min(f.size(-1) for f in skip_feats)
      skip_feats = [f[..., :min_t] for f in skip_feats]

      z = torch.stack(skip_feats, dim=0).sum(dim=0)

      z = self.tcn(z)
      z = self.proj_head(z)
      z = self.bn(z)

      return z

"""# Worker

Encoder 출력: z = (B, worker_input_dim, T')

### 공통 FFN Worker Head
"""

class RegressionWorker(nn.Module):
  def __init__(
      self,
      in_dim,
      out_dim,
      hidden_dim=256
  ):
      super().__init__()
      self.ffn = nn.Sequential(
          nn.Conv1d(in_dim, hidden_dim, kernel_size=1),
          nn.PReLU(hidden_dim),
          nn.Conv1d(hidden_dim, out_dim, kernel_size=1)
      )

  def forward(self, z, target):
      """
      z:      (B, in_dim, T_enc)
      target: (B, out_dim, T_target)
      """
      T_enc = z.size(-1)
      # Temporal alignment(interpolate)

      if target.size(-1) != T_enc:
            target = F.interpolate(
                target,
                size=T_enc,
                mode="linear",
                align_corners=False
            )

      # Frame-wise regression
      pred = self.ffn(z) # (B, out_dim, T_enc)

      return pred, target

"""### Multi-Worker Module(MFCC/CQT/Wav2Vec2)"""

class MultiRegressionWorkers(nn.Module):
  def __init__(
      self,
      worker_input_dim=256,
      mfcc_dim=40,
      cqt_dim=84,
      w2v2_dim=256
  ):
      super().__init__()

      self.mfcc_worker = RegressionWorker(
          in_dim=worker_input_dim,
          out_dim=mfcc_dim
      )

      self.cqt_worker = RegressionWorker(
          in_dim=worker_input_dim,
          out_dim=cqt_dim
      )

      self.w2v2_worker = RegressionWorker(
          in_dim=worker_input_dim,
          out_dim=w2v2_dim
      )

  def forward(self, z, targets):
      """
        z: (B, worker_input_dim, T_enc)
        targets:
          mfcc:     (B, 40, T_mfcc)
          cqt:      (B, 84, T_cqt)
          wav2vec2: (B, T_w2v, 768) or (B, 768, T_w2v)
      """
      outputs = {}

      # MFCC
      pred_mfcc, tgt_mfcc = self.mfcc_worker(z, targets["mfcc"])
      outputs["mfcc"] = (pred_mfcc, tgt_mfcc)

      # CQT
      pred_cqt, tgt_cqt = self.cqt_worker(z, targets["cqt"])
      outputs["cqt"] = (pred_cqt, tgt_cqt)

      # Wav2Vec2
      w2v2 = targets["wav2vec2"]

      if w2v2 is not None:
          # (B, T, 768) → (B, 768, T)
          if w2v2.dim() == 3 and w2v2.size(-1) == 768:
              w2v2 = w2v2.transpose(1, 2)

          pred_w2v, tgt_w2v = self.w2v2_worker(z, w2v2)
          outputs["wav2vec2"] = (pred_w2v, tgt_w2v)
      else:
          outputs["wav2vec2"] = None

      return outputs

"""이 시점에서 반드시 성립해야하는 shape

Encoder(x) → z: (B, 256, T')

MFCC target:      (B, 40,  T_mfcc)
MFCC prediction:  (B, 40,  T_mfcc)

CQT target:       (B, 84,  T_cqt)
CQT prediction:   (B, 84,  T_cqt)

wav2vec2 target:  (B, 768, T_w2v)
wav2vec2 pred:    (B, 768, T_w2v)

# Pretrain
"""

from torch.optim import Adam
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

def polynomial_decay_lr(optimizer, epoch, max_epoch, init_lr, power=1.0):
  lr = init_lr * (1 - epoch / max_epoch) ** power
  for param_group in optimizer.param_groups:
    param_group["lr"] = lr
  return lr

class PretrainModel(nn.Module):
  def __init__(
      self,
      encoder,
      workers,
      proj_hidden=256,
      worker_input_dim=256
  ):
      super().__init__()
      self.encoder = encoder
      self.proj = nn.Sequential(
          nn.Conv1d(encoder.embedding_dim, proj_hidden, kernel_size=1),
          nn.ReLU(),
          nn.Conv1d(proj_hidden, worker_input_dim, kernel_size=1)
      )
      self.workers = workers #{"mfcc": ... , "cqt": ..., "w2v2": ...}

  def forward(self, x, targets):
    """
    x: (B, 1, T) distorted waveform
    targets: dict of regression targets

    return:
      outputs: {
        "mfcc": (pred, target),
        "cqt": (pred, target),
        "wav2vec2": (pred, target)
      }
      z: (B, D, T_enc)
    """
    z = self.encoder(x)
    z = self.proj(z)

    outputs = self.workers(z, targets)

    return outputs, z

"""Encoder / projection: representation 생성

Workers: target별 temporal alignment + regression

Loss 계산은 training loop에서

### Train Loop
"""

def encoder_train(
    model,
    dataloader,
    target_dir,
    optimizer,
    device,
    num_epochs=100,
    init_lr=1e-3,
    lambda_mfcc=1.0,
    lambda_cqt=1.0,
    lambda_w2v2=1.0,
    writer=None
):
    model.train()

    mse_loss = nn.MSELoss()
    l1_loss = nn.SmoothL1Loss()

    global_step = 0

    for epoch in range(num_epochs):
        lr = polynomial_decay_lr(
            optimizer,
            epoch,
            num_epochs,
            init_lr
        )

        for batch in dataloader:
            distorted = batch["distorted"].to(device)  # (B, T)
            file_ids = batch["file_id"]

            distorted = distorted.unsqueeze(1)  # (B, 1, T)

            # ---- load targets ----
            targets = {"mfcc": [], "cqt": [], "wav2vec2": []}
            # targets = {"mfcc": [], "cqt": []}

            for fid in file_ids:
                t = torch.load(
                    os.path.join(target_dir, f"{fid}.pt"),
                    map_location="cpu"
                )
                targets["mfcc"].append(t["mfcc"])
                targets["cqt"].append(t["cqt"])

                if "wav2vec2" in t and t["wav2vec2"] is not None:
                  targets["wav2vec2"].append(t["wav2vec2"])

            targets["mfcc"] = torch.stack(targets["mfcc"]).to(device)
            targets["cqt"] = torch.stack(targets["cqt"]).to(device)

            if len(targets["wav2vec2"]) > 0:
              w2v2 = targets["wav2vec2"]
              if w2v2[0].dim() == 2:
                  w2v2 = [x.transpose(0, 1) for x in w2v2]
              targets["wav2vec2"] = torch.stack(w2v2).to(device)
            else:
              targets["wav2vec2"] = None

            # ---- forward ----
            outputs, z = model(distorted, targets)

            # ---- losses ----
            loss = 0.0
            loss_count = 0

            # MFCC (always on)
            loss_mfcc = mse_loss(*outputs["mfcc"])
            loss += lambda_mfcc * loss_mfcc
            loss_count += 1

            # CQT (always on)
            loss_cqt = mse_loss(*outputs["cqt"])
            loss += lambda_cqt * loss_cqt
            loss_count += 1

            # wav2vec2 (optional)
            if outputs["wav2vec2"] is not None:
                loss_w2v2 = l1_loss(*outputs["wav2vec2"])
                loss += lambda_w2v2 * loss_w2v2
                loss_count += 1
            else:
                loss_w2v2 = None  # logging 대비

            # normalize
            loss = loss / loss_count

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # ---- logging ----
            if writer is not None:
              writer.add_scalar("loss/total", loss.item(), global_step)
              writer.add_scalar("loss/mfcc", loss_mfcc.item(), global_step)
              writer.add_scalar("loss/cqt", loss_cqt.item(), global_step)

              if loss_w2v2 is not None:
                  writer.add_scalar("loss/wav2vec2", loss_w2v2.item(), global_step)

              writer.add_scalar("lr", lr, global_step)

            global_step += 1

        print(
            f"[Epoch {epoch+1}/{num_epochs}] "
            f"Loss: {loss.item():.4f} | LR: {lr:.6f}"
        )

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----- Encoder -----
print("[1/6] Building Encoder")
encoder = Encoder(
    embedding_dim=256
    # channel_list, kernel_list, stride_list 등
).to(device)

# ----- Workers -----
print("[2/6] Building Workers")
workers = MultiRegressionWorkers(
    worker_input_dim=256,
    mfcc_dim=40,
    cqt_dim=84,
    w2v2_dim=256
).to(device)

# ----- Pretrain Model -----
print("[3/6] Building Pretrain Model")
model = PretrainModel(
    encoder=encoder,
    workers=workers,
    proj_hidden=256,
    worker_input_dim=256
).to(device)

# ----- Distortion -----
print("[4/6] Loading & filtering RIRs")
ir_list = build_filtered_ir_list("data/reverb/RIRS_NOISES")
print("[5/6] Loading MUSAN noise dataset")
noise_dataset = MusanNoiseDataset("data/noise/musan/noise")

distortion = SpeechDistortionModule(
    ir_list=ir_list,
    noise_dataset=noise_dataset,
    sample_rate=16000
)

# ----- Dataset -----
print("[6/6] Building Dataset & DataLoader")
dataset = PretrainWaveDataset(
    save_dir="data/Track1.2/train/chunked/stage1",
    distortion=distortion,
    apply_distortion=True
)

loader = DataLoader(
    dataset,
    batch_size=16,
    shuffle=True,
    num_workers=0,
    pin_memory=True
)

optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3
)

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter(log_dir="runs/ssad_pretrain")

encoder_train(
    model=model,
    dataloader=loader,
    target_dir="pretrain_targets",
    optimizer=optimizer,
    device=device,
    num_epochs=100,
    init_lr=1e-3,
    lambda_mfcc=1.0,
    lambda_cqt=1.0,
    lambda_w2v2=1.0,
    writer=writer
)

# ===============================
# Save pretrained encoder (FINAL)
# ===============================

save_path = "checkpoints/pretrain_encoder_final.pt"
os.makedirs(os.path.dirname(save_path), exist_ok=True)

torch.save(
    {
        "encoder": model.encoder.state_dict(),
        "proj": model.proj.state_dict(),
        "workers": model.workers.state_dict(),  # 선택
        "optimizer": optimizer.state_dict(),    # 선택
        "config": {
            "embedding_dim": 256,
            "worker_input_dim": 256,
            "mfcc_dim": 40,
            "cqt_dim": 84,
            # "w2v2_dim": 768,
        }
    },
    save_path
)

print(f"[INFO] Pretrained encoder saved to {save_path}")

! cat /proc/cpuinfo

!nvidia-smi